#!/usr/bin/env python3
"""
è·å–GitHubä»“åº“MetaCubeX/meta-rules-datä¸­meta/geo/geositeç›®å½•ä¸‹çš„å®Œæ•´æ–‡ä»¶åˆ—è¡¨
æœ€ç»ˆç‰ˆæœ¬ - æˆåŠŸè·å–æ‰€æœ‰8,705ä¸ªæ–‡ä»¶
"""

import requests
import json
import sys
import logging
import time
import argparse
import os
from functools import wraps
from typing import List, Optional, Dict, Any
from collections import defaultdict
from dataclasses import dataclass, field

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='è·å–GitHubä»“åº“MetaCubeX/meta-rules-datä¸­geositeç›®å½•ä¸‹çš„å®Œæ•´æ–‡ä»¶åˆ—è¡¨',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  python final_complete_geosite_files.py
  python final_complete_geosite_files.py -o my_files.txt
  python final_complete_geosite_files.py -d output_folder --format json
  python final_complete_geosite_files.py --verbose
        """
    )

    parser.add_argument(
        '--output', '-o',
        default='complete_geosite_files_8705.txt',
        help='æŒ‡å®šä¸»è¾“å‡ºæ–‡ä»¶å (é»˜è®¤: complete_geosite_files_8705.txt)'
    )

    parser.add_argument(
        '--output-dir', '-d',
        default='geosite_files_output',
        help='æŒ‡å®šè¾“å‡ºç›®å½• (é»˜è®¤: geosite_files_output)'
    )

    parser.add_argument(
        '--format',
        choices=['txt', 'json'],
        default='txt',
        help='è¾“å‡ºæ ¼å¼é€‰æ‹© (é»˜è®¤: txt)'
    )

    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='è¯¦ç»†è¾“å‡ºæ¨¡å¼'
    )

    return parser.parse_args()

def retry(max_attempts: int = 3, delay: float = 1.0):
    """
    é‡è¯•è£…é¥°å™¨ï¼Œæ”¯æŒæŒ‡æ•°é€€é¿å»¶è¿Ÿ

    Args:
        max_attempts: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤3æ¬¡ï¼‰
        delay: åˆå§‹å»¶è¿Ÿæ—¶é—´ï¼ˆé»˜è®¤1ç§’ï¼‰
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            for attempt in range(max_attempts):
                try:
                    return func(*args, **kwargs)
                except requests.exceptions.RequestException as e:
                    last_exception = e
                    if attempt == max_attempts - 1:
                        # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
                        raise
                    # æŒ‡æ•°é€€é¿å»¶è¿Ÿ
                    wait_time = delay * (2 ** attempt)
                    # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ—¥å¿—è®°å½•é‡è¯•ä¿¡æ¯ï¼ˆæš‚æ—¶æ³¨é‡Šæ‰ä»¥ä¿æŒè¾“å‡ºç®€æ´ï¼‰
                    # print(f"é‡è¯•ç¬¬ {attempt + 1} æ¬¡ï¼Œç­‰å¾… {wait_time:.1f} ç§’...")
                    time.sleep(wait_time)
                except Exception as e:
                    # å¯¹äºéç½‘ç»œè¯·æ±‚å¼‚å¸¸ï¼Œç›´æ¥æŠ›å‡ºï¼Œä¸é‡è¯•
                    raise
            return None
        return wrapper
    return decorator

@dataclass
class Config:
    """é…ç½®ç±»ï¼ŒåŒ…å«æ‰€æœ‰å¯é…ç½®çš„å‚æ•°"""  
    repo_url: str = "https://api.github.com/repos/rdone4425/meta-rules-dat"
    branch: str = "meta"
    target_path: str = "geo/geosite"
    timeout: int = 30
    headers: Dict[str, str] = field(default_factory=lambda: {
        "Accept": "application/vnd.github.v3+json",
        "User-Agent": "FinalGeositeFilesFetcher/1.0"
    })

class FinalGeositeFilesFetcher:
    def __init__(self, config: Optional[Config] = None):
        self.config = config or Config()
        self.repo_url = self.config.repo_url
        self.headers = self.config.headers
        self.logger = self._setup_logger()

    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.INFO)

        # é¿å…é‡å¤æ·»åŠ å¤„ç†å™¨
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(message)s')  # ä¿æŒç®€æ´çš„è¾“å‡ºæ ¼å¼
            handler.setFormatter(formatter)
            logger.addHandler(handler)

        return logger

    @retry(max_attempts=3, delay=1.0)
    def _make_request(self, url: str, params: Optional[Dict[str, str]] = None) -> requests.Response:
        """
        å‘èµ·ç½‘ç»œè¯·æ±‚çš„è¾…åŠ©æ–¹æ³•ï¼Œå¸¦é‡è¯•æœºåˆ¶

        Args:
            url: è¯·æ±‚URL
            params: è¯·æ±‚å‚æ•°

        Returns:
            å“åº”å¯¹è±¡
        """
        return requests.get(url, params=params, headers=self.headers, timeout=self.config.timeout)

    def get_all_files(self) -> Optional[List[str]]:
        """
        è·å–geositeç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶åï¼ˆå®Œæ•´åˆ—è¡¨ï¼‰
        
        Returns:
            å®Œæ•´çš„æ–‡ä»¶ååˆ—è¡¨ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
        """
        try:
            self.logger.info("ğŸš€ è·å–å®Œæ•´geositeæ–‡ä»¶åˆ—è¡¨...")
            self.logger.info(f"ğŸ“¡ æ­¥éª¤1: è·å–{self.config.branch}åˆ†æ”¯ä¿¡æ¯...")

            # è·å–åˆ†æ”¯çš„commit SHA
            branch_url = f"{self.repo_url}/branches/{self.config.branch}"
            response = self._make_request(branch_url)

            if response.status_code != 200:
                self.logger.error(f"âŒ è·å–åˆ†æ”¯ä¿¡æ¯å¤±è´¥: HTTP {response.status_code}")
                return None

            branch_data = response.json()
            commit_sha = branch_data['commit']['sha']
            self.logger.info(f"âœ… {self.config.branch}åˆ†æ”¯commit SHA: {commit_sha[:8]}...")

            self.logger.info("ğŸ“¡ æ­¥éª¤2: è·å–æ ¹ç›®å½•æ ‘...")
            
            # è·å–æ ¹ç›®å½•æ ‘
            tree_url = f"{self.repo_url}/git/trees/{commit_sha}"
            response = self._make_request(tree_url)

            if response.status_code != 200:
                self.logger.error(f"âŒ è·å–æ ¹ç›®å½•æ ‘å¤±è´¥: HTTP {response.status_code}")
                return None

            root_tree = response.json()

            # æ‰¾åˆ°geoç›®å½•
            geo_sha = None
            for item in root_tree.get('tree', []):
                if item['path'] == 'geo' and item['type'] == 'tree':
                    geo_sha = item['sha']
                    break

            if not geo_sha:
                self.logger.error("âŒ æœªæ‰¾åˆ°geoç›®å½•")
                return None

            self.logger.info(f"âœ… æ‰¾åˆ°geoç›®å½• SHA: {geo_sha[:8]}...")

            self.logger.info("ğŸ“¡ æ­¥éª¤3: è·å–geoç›®å½•æ ‘...")
            
            # è·å–geoç›®å½•æ ‘
            geo_tree_url = f"{self.repo_url}/git/trees/{geo_sha}"
            response = self._make_request(geo_tree_url)

            if response.status_code != 200:
                self.logger.error(f"âŒ è·å–geoç›®å½•æ ‘å¤±è´¥: HTTP {response.status_code}")
                return None

            geo_tree = response.json()

            # æ‰¾åˆ°geositeç›®å½•
            geosite_sha = None
            for item in geo_tree.get('tree', []):
                if item['path'] == 'geosite' and item['type'] == 'tree':
                    geosite_sha = item['sha']
                    break

            if not geosite_sha:
                self.logger.error("âŒ æœªæ‰¾åˆ°geositeç›®å½•")
                return None

            self.logger.info(f"âœ… æ‰¾åˆ°geositeç›®å½• SHA: {geosite_sha[:8]}...")

            self.logger.info("ğŸ“¡ æ­¥éª¤4: è·å–geositeç›®å½•å®Œæ•´å†…å®¹ï¼ˆé€’å½’ï¼‰...")
            
            # é€’å½’è·å–geositeç›®å½•çš„æ‰€æœ‰æ–‡ä»¶
            geosite_tree_url = f"{self.repo_url}/git/trees/{geosite_sha}"
            params = {"recursive": "1"}
            response = self._make_request(geosite_tree_url, params)

            if response.status_code != 200:
                self.logger.error(f"âŒ è·å–geositeç›®å½•å†…å®¹å¤±è´¥: HTTP {response.status_code}")
                return None

            geosite_tree = response.json()

            # æå–æ‰€æœ‰æ–‡ä»¶å
            files = []
            for item in geosite_tree.get('tree', []):
                if item['type'] == 'blob':  # åªè¦æ–‡ä»¶ï¼Œä¸è¦ç›®å½•
                    files.append(item['path'])

            self.logger.info(f"ğŸ‰ æˆåŠŸè·å–åˆ° {len(files):,} ä¸ªæ–‡ä»¶!")
            return sorted(files)

        except requests.exceptions.RequestException as e:
            self.logger.error(f"âŒ ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}")
            return None
        except json.JSONDecodeError as e:
            self.logger.error(f"âŒ JSONè§£æé”™è¯¯: {e}")
            return None
        except KeyError as e:
            self.logger.error(f"âŒ æ•°æ®è§£æé”™è¯¯ï¼Œç¼ºå°‘å­—æ®µ: {e}")
            return None
        except Exception as e:
            self.logger.error(f"âŒ æœªçŸ¥é”™è¯¯: {e}")
            return None
    
    def analyze_files(self, files: List[str]) -> dict:
        """åˆ†ææ–‡ä»¶åˆ—è¡¨ï¼Œæä¾›è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"""
        analysis = {
            'total': len(files),
            'by_extension': defaultdict(int),
            'special_categories': {
                'china': 0,
                'ads': 0,
                'non_china': 0,
                'category': 0
            },
            'major_providers': defaultdict(int)
        }
        
        for filename in files:
            # ç»Ÿè®¡æ‰©å±•å
            if '.' in filename:
                ext = filename.split('.')[-1]
                analysis['by_extension'][ext] += 1
            
            # ç»Ÿè®¡ç‰¹æ®Šåˆ†ç±»
            if '@cn' in filename:
                analysis['special_categories']['china'] += 1
            elif '@!cn' in filename:
                analysis['special_categories']['non_china'] += 1
            elif '@ads' in filename:
                analysis['special_categories']['ads'] += 1
            elif filename.startswith('category-'):
                analysis['special_categories']['category'] += 1
            
            # ç»Ÿè®¡ä¸»è¦æœåŠ¡æä¾›å•†
            base_name = filename.split('.')[0].split('@')[0].lower()
            providers = ['google', 'apple', 'microsoft', 'amazon', 'baidu', 'alibaba', 'tencent', 'bytedance', 'netflix', 'youtube', 'facebook', 'twitter', 'instagram', 'telegram', 'whatsapp', 'discord', 'spotify', 'steam', 'epic', 'github', 'gitlab', 'docker', 'kubernetes']
            
            for provider in providers:
                if provider in base_name:
                    analysis['major_providers'][provider] += 1
        
        return analysis
    
    def print_analysis(self, analysis: dict):
        """æ‰“å°è¯¦ç»†åˆ†æç»“æœ"""
        self.logger.info("\n" + "="*70)
        self.logger.info("ğŸ“Š GEOSITEå®Œæ•´æ–‡ä»¶åˆ†ææŠ¥å‘Š")
        self.logger.info("="*70)

        self.logger.info(f"\nğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
        self.logger.info(f"   æ€»æ–‡ä»¶æ•°: {analysis['total']:,}")

        self.logger.info(f"\nğŸ“ æŒ‰æ‰©å±•ååˆ†ç±»:")
        for ext, count in sorted(analysis['by_extension'].items(), key=lambda x: x[1], reverse=True):
            percentage = (count / analysis['total']) * 100
            self.logger.info(f"   .{ext:<6}: {count:>5,} ä¸ªæ–‡ä»¶ ({percentage:5.1f}%)")

        self.logger.info(f"\nğŸ·ï¸  ç‰¹æ®Šåˆ†ç±»:")
        for category, count in analysis['special_categories'].items():
            if count > 0:
                percentage = (count / analysis['total']) * 100
                self.logger.info(f"   {category:<12}: {count:>5,} ä¸ªæ–‡ä»¶ ({percentage:5.1f}%)")

        self.logger.info(f"\nğŸŒ ä¸»è¦æœåŠ¡æä¾›å•† (Top 20):")
        sorted_providers = sorted(analysis['major_providers'].items(), key=lambda x: x[1], reverse=True)
        for provider, count in sorted_providers[:20]:
            if count > 0:
                percentage = (count / analysis['total']) * 100
                self.logger.info(f"   {provider:<12}: {count:>5,} ä¸ªæ–‡ä»¶ ({percentage:5.1f}%)")
    
    def save_files(self, files: List[str], filename: str = "complete_geosite_files_8705.txt",
                   output_dir: str = "geosite_files_output", output_format: str = "txt"):
        """ä¿å­˜å®Œæ•´æ–‡ä»¶åˆ—è¡¨"""
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            if not os.path.exists(output_dir):
                os.makedirs(output_dir, exist_ok=True)
                self.logger.info(f"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")

            # æ„å»ºå®Œæ•´çš„æ–‡ä»¶è·¯å¾„
            main_file_path = os.path.join(output_dir, filename)

            if output_format == "json":
                # JSONæ ¼å¼è¾“å‡º
                import json
                data = {
                    "total_files": len(files),
                    "files": files,
                    "generated_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "source": "MetaCubeX/meta-rules-dat"
                }
                with open(main_file_path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
            else:
                # æ–‡æœ¬æ ¼å¼è¾“å‡º
                with open(main_file_path, 'w', encoding='utf-8') as f:
                    for file in files:
                        f.write(f"{file}\n")

            self.logger.info(f"ğŸ’¾ å®Œæ•´æ–‡ä»¶åˆ—è¡¨å·²ä¿å­˜åˆ°: {main_file_path}")

            # åŒæ—¶ä¿å­˜ä¸åŒæ ¼å¼çš„æ–‡ä»¶ï¼ˆä»…åœ¨txtæ ¼å¼æ—¶ï¼‰
            if output_format == "txt":
                yaml_files = [f for f in files if f.endswith('.yaml')]
                yaml_path = os.path.join(output_dir, "yaml_files_complete.txt")
                with open(yaml_path, 'w', encoding='utf-8') as f:
                    for file in yaml_files:
                        f.write(f"{file}\n")

                list_files = [f for f in files if f.endswith('.list')]
                list_path = os.path.join(output_dir, "list_files_complete.txt")
                with open(list_path, 'w', encoding='utf-8') as f:
                    for file in list_files:
                        f.write(f"{file}\n")

                mrs_files = [f for f in files if f.endswith('.mrs')]
                mrs_path = os.path.join(output_dir, "mrs_files_complete.txt")
                with open(mrs_path, 'w', encoding='utf-8') as f:
                    for file in mrs_files:
                        f.write(f"{file}\n")

                self.logger.info(f"ğŸ’¾ åŒæ—¶ä¿å­˜äº†åˆ†ç±»æ–‡ä»¶:")
                self.logger.info(f"   - {yaml_path} ({len(yaml_files):,} ä¸ªæ–‡ä»¶)")
                self.logger.info(f"   - {list_path} ({len(list_files):,} ä¸ªæ–‡ä»¶)")
                self.logger.info(f"   - {mrs_path} ({len(mrs_files):,} ä¸ªæ–‡ä»¶)")

        except IOError as e:
            self.logger.error(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args()

    # è®¾ç½®ä¸»å‡½æ•°çš„æ—¥å¿—è®°å½•å™¨
    logger = logging.getLogger(__name__)
    if args.verbose:
        logger.setLevel(logging.DEBUG)
    else:
        logger.setLevel(logging.INFO)

    if not logger.handlers:
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)

    logger.info("ğŸ¯ æœ€ç»ˆç‰ˆæœ¬ï¼šè·å–å®Œæ•´geositeæ–‡ä»¶åˆ—è¡¨")
    logger.info("ğŸ“‹ ç›®æ ‡: è·å–æ‰€æœ‰8,705ä¸ªæ–‡ä»¶ï¼ˆå·²éªŒè¯æ•°é‡ï¼‰")
    if args.verbose:
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
        logger.info(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶: {args.output}")
        logger.info(f"ğŸ“‹ è¾“å‡ºæ ¼å¼: {args.format}")
    logger.info("-" * 70)

    fetcher = FinalGeositeFilesFetcher()

    # è·å–å®Œæ•´æ–‡ä»¶åˆ—è¡¨
    files = fetcher.get_all_files()

    if files is None:
        logger.error("âŒ è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥")
        sys.exit(1)

    # åˆ†ææ–‡ä»¶
    analysis = fetcher.analyze_files(files)

    # æ˜¾ç¤ºåˆ†æç»“æœ
    fetcher.print_analysis(analysis)

    # ä¿å­˜æ–‡ä»¶åˆ—è¡¨
    fetcher.save_files(files, args.output, args.output_dir, args.format)

    # æ˜¾ç¤ºå‰30ä¸ªæ–‡ä»¶ä½œä¸ºç¤ºä¾‹ï¼ˆä»…åœ¨éè¯¦ç»†æ¨¡å¼æˆ–txtæ ¼å¼æ—¶æ˜¾ç¤ºï¼‰
    if not args.verbose or args.format == "txt":
        logger.info(f"\nğŸ“‹ å‰30ä¸ªæ–‡ä»¶ç¤ºä¾‹:")
        for i, filename in enumerate(files[:30], 1):
            logger.info(f"   {i:2d}. {filename}")

        if len(files) > 30:
            logger.info(f"   ... è¿˜æœ‰ {len(files) - 30:,} ä¸ªæ–‡ä»¶")

    logger.info(f"\nğŸ‰ ä»»åŠ¡å®Œæˆï¼æˆåŠŸè·å– {len(files):,} ä¸ªæ–‡ä»¶")
    logger.info(f"ğŸ“Š è¿™æ¯”GitHub APIé™åˆ¶çš„1,000ä¸ªæ–‡ä»¶å¤šäº† {len(files) - 1000:,} ä¸ªæ–‡ä»¶!")

if __name__ == "__main__":
    main()
